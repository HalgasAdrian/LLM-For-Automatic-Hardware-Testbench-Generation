{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HalgasAdrian/LLM-For-Automatic-Hardware-Testbench-Generation/blob/main/notebooks/train_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# LLM Testbench Generation Training on Colab This notebook trains the model on Google Colab with GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOwjhyHGa2L_",
        "outputId": "1af40c5c-af99-4c19-963e-a33bd4ed1fdb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 12 02:19:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             42W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_header"
      },
      "source": [
        "## 3. Upload and Extract Project Files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Upload from local computer\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Select llm_testbench_colab.zip\n",
        "\n",
        "# Extract\n",
        "!unzip -q llm_testbench_colab.zip\n",
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "87JYqbv5bLCm",
        "outputId": "23843fc5-4485-4f1b-dc96-b24a5dfba6f1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-95f08fd2-8dac-40cb-840d-ab67a977ad9c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-95f08fd2-8dac-40cb-840d-ab67a977ad9c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving llm_testbench_colab.zip to llm_testbench_colab (2).zip\n",
            "replace requirements.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace .env? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace requirements_colab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace utils/data_utils.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace utils/verilog_utils.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace utils/__pycache__/verilog_utils.cpython-311.pyc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace utils/__pycache__/data_utils.cpython-311.pyc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace configs/config.yaml? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace scripts/prepare_for_colab.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace scripts/download_data.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace scripts/train.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace scripts/analyze_dataset.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace scripts/process_data.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace scripts/evaluate.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace scripts/check_imports.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace scripts/augment_data.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace data/processed/.gitkeep? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace data/processed/new_examples.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace data/processed/test/test.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace data/processed/train/train_original.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace data/processed/train/train.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace data/processed/val/val.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace notebooks/train_on_colab.ipynb? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "total 229520\n",
            "drwxr-xr-x 1 root root      4096 Aug 12 02:19  .\n",
            "drwxr-xr-x 1 root root      4096 Aug 12 00:18  ..\n",
            "drwxr-xr-x 4 root root      4096 Aug  8 20:22  .config\n",
            "drwxr-xr-x 2 root root      4096 Aug 12 02:19  configs\n",
            "drwxr-xr-x 4 root root      4096 Aug 12 00:57  data\n",
            "-rw-r--r-- 1 root root       177 Aug 10 23:07  .env\n",
            "-rw-r--r-- 1 root root     82408 Aug 12 01:29 'llm_testbench_colab (1).zip'\n",
            "-rw-r--r-- 1 root root    124125 Aug 12 02:19 'llm_testbench_colab (2).zip'\n",
            "-rw-r--r-- 1 root root     80617 Aug 12 00:45  llm_testbench_colab.zip\n",
            "drwxr-xr-x 3 root root      4096 Aug 12 00:48  models\n",
            "drwxr-xr-x 2 root root      4096 Aug 12 02:20  notebooks\n",
            "-rw-r--r-- 1 root root       159 Aug 11 20:42  requirements_colab.txt\n",
            "-rw-r--r-- 1 root root       736 Aug 10 23:33  requirements.txt\n",
            "drwxr-xr-x 1 root root      4096 Aug  8 20:22  sample_data\n",
            "drwxr-xr-x 2 root root      4096 Aug 12 02:20  scripts\n",
            "-rw-r--r-- 1 root root 234675792 Aug 12 01:35  trained_model.zip\n",
            "drwxr-xr-x 3 root root      4096 Aug 12 02:19  utils\n",
            "drwxr-xr-x 6 root root      4096 Aug 12 01:31  wandb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_header"
      },
      "source": [
        "## 4. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requirements\n",
        "!pip install -q -r requirements_colab.txt\n",
        "\n",
        "# Verify installations\n",
        "!pip list | grep -E \"torch|transformers|peft|bitsandbytes\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsBAILCsbSE0",
        "outputId": "2a88413e-c0b7-4258-c3c8-6366583c7b48"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bitsandbytes                          0.47.0\n",
            "peft                                  0.17.0\n",
            "sentence-transformers                 5.0.0\n",
            "torch                                 2.6.0+cu124\n",
            "torchao                               0.10.0\n",
            "torchaudio                            2.6.0+cu124\n",
            "torchdata                             0.11.0\n",
            "torchsummary                          1.5.1\n",
            "torchtune                             0.6.1\n",
            "torchvision                           0.21.0+cu124\n",
            "transformers                          4.55.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wandb_header"
      },
      "source": [
        "## 5. Configure Weights & Biases (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_wandb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "8f8f6900-53c5-4b7d-dbf8-82cfd154a750"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">colab-training-run</strong> at: <a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/fexqkzer' target=\"_blank\">https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/fexqkzer</a><br> View project at: <a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen' target=\"_blank\">https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250812_013044-fexqkzer/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250812_022113-jv41edum</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/jv41edum' target=\"_blank\">colab-training-run</a></strong> to <a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen' target=\"_blank\">https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/jv41edum' target=\"_blank\">https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/jv41edum</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/jv41edum?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fb4599be110>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Set up wandb for experiment tracking\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "# Set API key directly\n",
        "os.environ['WANDB_API_KEY'] = 'b29c0d2102aa226ead1fded36b786769f969d4f7'\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"llm-testbench-gen\",\n",
        "    name=\"colab-training-run\",\n",
        "    config={\n",
        "        \"model\": \"TinyLlama-1.1B\",\n",
        "        \"dataset\": \"AutoBench\",\n",
        "        \"epochs\": 10\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_header"
      },
      "source": [
        "## 6. Update Config for Colab GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "update_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3050bc-0182-4c29-aa1f-a30b71d2a815"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config updated for Colab GPU training with improvements\n",
            "Learning rate: 0.0001 (type: <class 'float'>)\n",
            "Optimizer: adamw_torch\n",
            "Epochs: 10\n",
            "LoRA rank: 32\n",
            "Effective batch size: 16\n",
            "Total training examples: Check your augmented dataset size\n"
          ]
        }
      ],
      "source": [
        "# Create optimized config for Colab\n",
        "import yaml\n",
        "\n",
        "# Load existing config\n",
        "with open('configs/config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Update for GPU training with IMPROVED settings\n",
        "config['model']['quantization']['load_in_4bit'] = True  # Enable 4-bit quantization\n",
        "config['training']['per_device_train_batch_size'] = 2   # Smaller batch for better learning\n",
        "config['training']['per_device_eval_batch_size'] = 2    # Match train batch size\n",
        "config['training']['gradient_accumulation_steps'] = 8   # Increased for effective batch of 16\n",
        "config['training']['fp16'] = True  # Enable mixed precision\n",
        "config['training']['num_train_epochs'] = 10  # Increased from 5\n",
        "\n",
        "# Adjusted learning parameters\n",
        "config['training']['learning_rate'] = 1e-4   # Lowered from 2e-4 for more stable learning\n",
        "config['training']['warmup_steps'] = 50      # Increased from 10 for gradual warmup\n",
        "config['training']['warmup_ratio'] = 0.03    # Keep this\n",
        "config['training']['max_grad_norm'] = 0.3    # Keep this for gradient clipping\n",
        "\n",
        "# Improved LoRA configuration\n",
        "config['model']['lora']['r'] = 32                          # Increased from 16\n",
        "config['model']['lora']['lora_alpha'] = 64                 # Scaled with r (2x)\n",
        "config['model']['lora']['target_modules'] = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Target all attention\n",
        "\n",
        "# Training schedule improvements\n",
        "config['training']['logging_steps'] = 2      # More frequent logging\n",
        "config['training']['save_steps'] = 20        # Save more frequently\n",
        "config['training']['eval_steps'] = 10        # Evaluate more often\n",
        "\n",
        "# Keep optimizer\n",
        "config['training']['optim'] = 'adamw_torch'\n",
        "\n",
        "# Save updated config\n",
        "with open('configs/config_colab.yaml', 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "print(\"Config updated for Colab GPU training with improvements\")\n",
        "print(f\"Learning rate: {config['training']['learning_rate']} (type: {type(config['training']['learning_rate'])})\")\n",
        "print(f\"Optimizer: {config['training']['optim']}\")\n",
        "print(f\"Epochs: {config['training']['num_train_epochs']}\")\n",
        "print(f\"LoRA rank: {config['model']['lora']['r']}\")\n",
        "print(f\"Effective batch size: {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
        "print(f\"Total training examples: Check your augmented dataset size\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_header"
      },
      "source": [
        "## 7. Run Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the train.py file directly in Colab\n",
        "!sed -i 's/evaluation_strategy=/eval_strategy=/g' /content/scripts/train.py\n",
        "\n",
        "# Verify the change was made\n",
        "!grep -n \"eval_strategy\" /content/scripts/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EewVbGdvfBD1",
        "outputId": "9a1127e9-dbb0-4138-ecb6-0cd1a6fb94a9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "159:        eval_strategy=\"steps\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_training",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3762902-bec4-49b8-aa97-ff64d68addb6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-12 02:21:30.424530: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-12 02:21:30.443953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754965290.465565   32214 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754965290.472220   32214 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754965290.490161   32214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754965290.490193   32214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754965290.490196   32214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754965290.490199   32214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-12 02:21:30.495197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhalgas-a\u001b[0m (\u001b[33mhalgas-a-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250812_022135-1bejm2sp\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrain-TinyLlama-1.1B-Chat-v1.0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/1bejm2sp\u001b[0m\n",
            "trainable params: 9,011,200 || all params: 1,109,059,584 || trainable%: 0.8125\n",
            "/content/scripts/train.py:217: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "  0% 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "{'loss': 0.8044, 'grad_norm': 0.5284559726715088, 'learning_rate': 0.0001, 'epoch': 0.84}\n",
            "{'loss': 0.6299, 'grad_norm': 0.45507827401161194, 'learning_rate': 0.0001, 'epoch': 1.42}\n",
            "{'loss': 0.7981, 'grad_norm': 0.5381877422332764, 'learning_rate': 0.0001, 'epoch': 2.0}\n",
            "{'loss': 0.5766, 'grad_norm': 0.3609564006328583, 'learning_rate': 0.0001, 'epoch': 2.84}\n",
            "{'loss': 0.7085, 'grad_norm': 0.3460436463356018, 'learning_rate': 0.0001, 'epoch': 3.42}\n",
            " 33% 10/30 [01:15<02:28,  7.43s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  6.27it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.6649975180625916, 'eval_runtime': 3.0712, 'eval_samples_per_second': 1.954, 'eval_steps_per_second': 0.977, 'epoch': 3.42}\n",
            " 33% 10/30 [01:18<02:28,  7.43s/it]\n",
            "100% 3/3 [00:02<00:00,  4.51it/s]\u001b[A\n",
            "{'loss': 0.6269, 'grad_norm': 0.4624393880367279, 'learning_rate': 0.0001, 'epoch': 4.0}\n",
            "{'loss': 0.4982, 'grad_norm': 0.37379196286201477, 'learning_rate': 0.0001, 'epoch': 4.84}\n",
            "{'loss': 0.4383, 'grad_norm': 0.5157803893089294, 'learning_rate': 0.0001, 'epoch': 5.42}\n",
            "{'loss': 0.3908, 'grad_norm': 0.39864033460617065, 'learning_rate': 0.0001, 'epoch': 6.0}\n",
            "{'loss': 0.3938, 'grad_norm': 0.41948097944259644, 'learning_rate': 0.0001, 'epoch': 6.84}\n",
            " 67% 20/30 [02:34<01:19,  8.00s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  6.29it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.4136737287044525, 'eval_runtime': 3.0747, 'eval_samples_per_second': 1.951, 'eval_steps_per_second': 0.976, 'epoch': 6.84}\n",
            " 67% 20/30 [02:37<01:19,  8.00s/it]\n",
            "100% 3/3 [00:02<00:00,  4.52it/s]\u001b[A\n",
            "{'loss': 0.2833, 'grad_norm': 0.44879618287086487, 'learning_rate': 0.0001, 'epoch': 7.42}\n",
            "{'loss': 0.2631, 'grad_norm': 1.0503941774368286, 'learning_rate': 0.0001, 'epoch': 8.0}\n",
            "{'loss': 0.2511, 'grad_norm': 0.45361074805259705, 'learning_rate': 0.0001, 'epoch': 8.84}\n",
            "{'loss': 0.2453, 'grad_norm': 0.3803783655166626, 'learning_rate': 0.0001, 'epoch': 9.42}\n",
            "{'loss': 0.2254, 'grad_norm': 0.6570450067520142, 'learning_rate': 0.0001, 'epoch': 10.0}\n",
            "100% 30/30 [03:47<00:00,  6.66s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  6.30it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.2180151790380478, 'eval_runtime': 3.0715, 'eval_samples_per_second': 1.953, 'eval_steps_per_second': 0.977, 'epoch': 10.0}\n",
            "100% 30/30 [03:50<00:00,  6.66s/it]\n",
            "100% 3/3 [00:02<00:00,  4.52it/s]\u001b[A\n",
            "{'train_runtime': 230.8028, 'train_samples_per_second': 1.646, 'train_steps_per_second': 0.13, 'train_loss': 0.4755790799856186, 'epoch': 10.0}\n",
            "100% 30/30 [03:50<00:00,  7.69s/it]\n",
            "\n",
            "==================================================\n",
            "Training Complete!\n",
            "==================================================\n",
            "Final loss: 0.4755790799856186\n",
            "Total training time: 230.8028 seconds\n",
            "\n",
            "Next step: Run 'python scripts/evaluate.py' to evaluate the model\n"
          ]
        }
      ],
      "source": [
        "# Set config file\n",
        "!cp configs/config_colab.yaml configs/config.yaml\n",
        "\n",
        "# Run training\n",
        "!python scripts/train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_header"
      },
      "source": [
        "## 8. Save Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compress_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3061f1e-812a-49ca-c16f-b512fc12c176"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: adapter_config.json (deflated 56%)\n",
            "updating: adapter_model.safetensors (deflated 8%)\n",
            "updating: chat_template.jinja (deflated 60%)\n",
            "updating: checkpoint-20/ (stored 0%)\n",
            "updating: checkpoint-20/tokenizer.model (deflated 55%)\n",
            "updating: checkpoint-20/rng_state.pth (deflated 25%)\n",
            "updating: checkpoint-20/tokenizer_config.json (deflated 69%)\n",
            "updating: checkpoint-20/scaler.pt (deflated 60%)\n",
            "updating: checkpoint-20/training_args.bin (deflated 51%)\n",
            "updating: checkpoint-20/README.md (deflated 66%)\n",
            "updating: checkpoint-20/optimizer.pt (deflated 8%)\n",
            "updating: checkpoint-20/special_tokens_map.json (deflated 73%)\n",
            "updating: checkpoint-20/adapter_config.json (deflated 56%)\n",
            "updating: checkpoint-20/chat_template.jinja (deflated 60%)\n",
            "updating: checkpoint-20/scheduler.pt (deflated 58%)\n",
            "updating: checkpoint-20/tokenizer.json (deflated 85%)\n",
            "updating: checkpoint-20/trainer_state.json (deflated 73%)\n",
            "updating: checkpoint-20/adapter_model.safetensors (deflated 8%)\n",
            "updating: checkpoint-30/ (stored 0%)\n",
            "updating: checkpoint-30/tokenizer.model (deflated 55%)\n",
            "updating: checkpoint-30/rng_state.pth (deflated 25%)\n",
            "updating: checkpoint-30/tokenizer_config.json (deflated 69%)\n",
            "updating: checkpoint-30/scaler.pt (deflated 60%)\n",
            "updating: checkpoint-30/training_args.bin (deflated 51%)\n",
            "updating: checkpoint-30/README.md (deflated 66%)\n",
            "updating: checkpoint-30/optimizer.pt (deflated 8%)\n",
            "updating: checkpoint-30/special_tokens_map.json (deflated 73%)\n",
            "updating: checkpoint-30/adapter_config.json (deflated 56%)\n",
            "updating: checkpoint-30/chat_template.jinja (deflated 60%)\n",
            "updating: checkpoint-30/scheduler.pt (deflated 58%)\n",
            "updating: checkpoint-30/tokenizer.json (deflated 85%)\n",
            "updating: checkpoint-30/trainer_state.json (deflated 76%)\n",
            "updating: checkpoint-30/adapter_model.safetensors (deflated 8%)\n",
            "updating: README.md (deflated 66%)\n",
            "updating: special_tokens_map.json (deflated 73%)\n",
            "updating: tokenizer_config.json (deflated 69%)\n",
            "updating: tokenizer.json (deflated 85%)\n",
            "updating: tokenizer.model (deflated 55%)\n",
            "updating: training_args.bin (deflated 51%)\n",
            "updating: training_results.json (deflated 33%)\n",
            "-rw-r--r-- 1 root root 224M Aug 12 02:26 trained_model.zip\n"
          ]
        }
      ],
      "source": [
        "# Compress the trained model\n",
        "!cd models/checkpoints && zip -r /content/trained_model.zip * && cd /content\n",
        "!ls -lh trained_model.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "98ee013f-7cb7-43c5-88f6-f4904951b43c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_da0d9612-ee3b-4753-bb51-6247bc9176f0\", \"trained_model.zip\", 234696539)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download to local computer\n",
        "from google.colab import files\n",
        "files.download('trained_model.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Install Icarus Verilog for Testing\n"
      ],
      "metadata": {
        "id": "ZqeA-KLIqClN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Icarus Verilog for compilation tests\n",
        "!apt-get update\n",
        "!apt-get install -y iverilog\n",
        "!iverilog -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_jBGOAFqCCC",
        "outputId": "eee99e8e-92d2-492d-f1bf-31d2dd174ffd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,270 kB]\n",
            "Fetched 1,399 kB in 1s (1,154 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "iverilog is already the newest version (11.0-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "iverilog: invalid option -- 'e'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Run Evaluation"
      ],
      "metadata": {
        "id": "UU_7U4WnqJg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the evaluation script if it doesn't exist\n",
        "!cp /content/scripts/evaluate.py /content/scripts/evaluate_colab.py || echo \"Creating new evaluation script\"\n",
        "\n",
        "# If evaluate.py doesn't exist, download it\n",
        "if not os.path.exists('/content/scripts/evaluate.py'):\n",
        "    print(\"Downloading evaluation script...\")\n",
        "    # You can paste the evaluate.py content here or upload it"
      ],
      "metadata": {
        "id": "RVyPhTRVqKat"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation on the trained model\n",
        "!python scripts/evaluate.py\n",
        "\n",
        "# Check results\n",
        "!ls -la data/test_results/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_K1LfaIrHF-",
        "outputId": "ab3e7c19-c353-4d13-ff83-744ac44b015d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-12 02:27:01.084483: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-12 02:27:01.101904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754965621.123393   34422 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754965621.129839   34422 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754965621.146217   34422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754965621.146245   34422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754965621.146249   34422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754965621.146253   34422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-12 02:27:01.151187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Evaluating:   0% 0/8 [00:00<?, ?it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
            "Evaluating: 100% 8/8 [08:33<00:00, 64.22s/it]\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "Total examples evaluated: 8\n",
            "\n",
            "Success Rates:\n",
            "  Generation: 100.0%\n",
            "  Compilation: 0.0%\n",
            "  Simulation: 0.0%\n",
            "  Valid Syntax: 0.0%\n",
            "\n",
            "Component Presence:\n",
            "  timescale: 100.0%\n",
            "  module: 100.0%\n",
            "  initial: 100.0%\n",
            "  finish: 87.5%\n",
            "  display: 50.0%\n",
            "\n",
            "Detailed results saved to: data/test_results\n",
            "\n",
            "Next step: Run 'python scripts/generate.py --dut_file <your_design.v>' to generate testbenches for new designs\n",
            "total 116\n",
            "drwxr-xr-x 2 root root  4096 Aug 12 01:01 .\n",
            "drwxr-xr-x 4 root root  4096 Aug 12 00:57 ..\n",
            "-rw-r--r-- 1 root root 60386 Aug 12 02:35 evaluation_results.json\n",
            "-rw-r--r-- 1 root root   343 Aug 12 01:01 generated_tb_0_fixed.v\n",
            "-rw-r--r-- 1 root root  1122 Aug 12 02:35 generated_tb_0.v\n",
            "-rw-r--r-- 1 root root  1610 Aug 12 02:35 generated_tb_1.v\n",
            "-rw-r--r-- 1 root root  1591 Aug 12 01:01 generated_tb_2_fixed.v\n",
            "-rw-r--r-- 1 root root  3494 Aug 12 02:35 generated_tb_2.v\n",
            "-rw-r--r-- 1 root root   324 Aug 12 02:35 generated_tb_3.v\n",
            "-rw-r--r-- 1 root root  4253 Aug 12 02:35 generated_tb_4.v\n",
            "-rw-r--r-- 1 root root   532 Aug 12 02:35 generated_tb_5.v\n",
            "-rw-r--r-- 1 root root  4059 Aug 12 02:35 generated_tb_6.v\n",
            "-rw-r--r-- 1 root root  6556 Aug 12 02:35 generated_tb_7.v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and display evaluation results\n",
        "import json\n",
        "\n",
        "with open('data/test_results/evaluation_results.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"=\"*50)\n",
        "for metric, value in results['metrics'].items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{metric}: {value:.2%}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xxv7DQOrJsA",
        "outputId": "c82e45c7-1646-4fd4-f6d0-ddc54e843e35"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics:\n",
            "==================================================\n",
            "total_examples: 8\n",
            "generation_success_rate: 100.00%\n",
            "compilation_success_rate: 0.00%\n",
            "simulation_success_rate: 0.00%\n",
            "syntax_valid_rate: 0.00%\n",
            "has_timescale_rate: 100.00%\n",
            "has_module_rate: 100.00%\n",
            "has_initial_rate: 100.00%\n",
            "has_finish_rate: 87.50%\n",
            "has_display_rate: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View a sample generated testbench\n",
        "import random\n",
        "\n",
        "# List generated files\n",
        "import glob\n",
        "tb_files = glob.glob('data/test_results/generated_tb_*.v')\n",
        "\n",
        "if tb_files:\n",
        "    # Pick a random one to display\n",
        "    sample_file = random.choice(tb_files)\n",
        "    print(f\"Viewing: {sample_file}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    with open(sample_file, 'r') as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(\"No generated testbenches found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2YFgpRArLns",
        "outputId": "8c9564be-c48b-4bcf-a5d1-52cb8d501f63"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Viewing: data/test_results/generated_tb_3.v\n",
            "============================================================\n",
            "`timescale 1ns / 1ps module tb;\n",
            "reg clk;\n",
            "reg d;\n",
            "reg ar;\n",
            "wire q;\n",
            "integer file, line;\n",
            "integer err_count = 0;\n",
            "top_module dut ( .clk, .d, .ar, .q );\n",
            "initial begin\n",
            "clk = 0;\n",
            "ar = 0;\n",
            "d = 0;\n",
            "q = 0;\n",
            "// Initial conditions #10 // Sync reset $finish;\n",
            "end#10 // Sync reset // Test Cases initial begin\n",
            "file = $time;\n",
            "#10 // Sync reset ar =\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress evaluation results\n",
        "!cd data/test_results && zip -r /content/evaluation_results.zip * && cd /content\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download('evaluation_results.zip')\n",
        "print(\"Evaluation results downloaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "SblGOIcdtw6w",
        "outputId": "9ae5f32d-8b54-4caa-e6db-7e265d34267d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: evaluation_results.json (deflated 92%)\n",
            "  adding: generated_tb_0_fixed.v (deflated 35%)\n",
            "  adding: generated_tb_0.v (deflated 59%)\n",
            "  adding: generated_tb_1.v (deflated 64%)\n",
            "  adding: generated_tb_2_fixed.v (deflated 67%)\n",
            "  adding: generated_tb_2.v (deflated 82%)\n",
            "  adding: generated_tb_3.v (deflated 41%)\n",
            "  adding: generated_tb_4.v (deflated 79%)\n",
            "  adding: generated_tb_5.v (deflated 45%)\n",
            "  adding: generated_tb_6.v (deflated 83%)\n",
            "  adding: generated_tb_7.v (deflated 87%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a1eb97df-a90e-411d-ae01-5a4017bc423f\", \"evaluation_results.zip\", 12009)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze training data quality\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Load training data\n",
        "with open('data/processed/train/train.jsonl', 'r') as f:\n",
        "    train_data = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"Total training examples: {len(train_data)}\")\n",
        "\n",
        "# Check for problematic patterns\n",
        "issues = {\n",
        "    'has_c_code': 0,\n",
        "    'has_initial_brace': 0,\n",
        "    'has_wrong_bit_width': 0,\n",
        "    'has_mixed_comments': 0,\n",
        "    'wire_assignment': 0\n",
        "}\n",
        "\n",
        "clean_examples = []\n",
        "problematic_indices = []\n",
        "\n",
        "for i, example in enumerate(train_data):\n",
        "    tb = example['testbench_code']\n",
        "    has_issue = False\n",
        "\n",
        "    # Check for C code\n",
        "    if 'int main' in tb or '#include' in tb:\n",
        "        issues['has_c_code'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    # Check for wrong initial syntax\n",
        "    if 'initial {' in tb:\n",
        "        issues['has_initial_brace'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    # Check for wrong bit widths\n",
        "    if re.search(r'\\d{2,}\\'b', tb):  # 10'b, 12'b, etc.\n",
        "        issues['has_wrong_bit_width'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    # Check for mixed comments/documentation\n",
        "    if '// Change:' in tb or '### Correctness:' in tb:\n",
        "        issues['has_mixed_comments'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    # Check for wire assignments in initial blocks\n",
        "    if re.search(r'initial.*?wire.*?=', tb, re.DOTALL):\n",
        "        issues['wire_assignment'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    if has_issue:\n",
        "        problematic_indices.append(i)\n",
        "    else:\n",
        "        clean_examples.append(example)\n",
        "\n",
        "print(\"\\nIssues found in training data:\")\n",
        "for issue, count in issues.items():\n",
        "    print(f\"  {issue}: {count} ({count/len(train_data)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClean examples: {len(clean_examples)} ({len(clean_examples)/len(train_data)*100:.1f}%)\")\n",
        "print(f\"Problematic examples: {len(problematic_indices)}\")\n",
        "\n",
        "# Save clean training data\n",
        "if clean_examples:\n",
        "    with open('data/processed/train/train_clean.jsonl', 'w') as f:\n",
        "        for example in clean_examples:\n",
        "            f.write(json.dumps(example) + '\\n')\n",
        "    print(f\"\\nClean training data saved to train_clean.jsonl\")"
      ],
      "metadata": {
        "id": "K0S_Ht7X-tPC",
        "outputId": "d22c7e7d-83a3-4ba2-e8d6-68044c3e58be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training examples: 38\n",
            "\n",
            "Issues found in training data:\n",
            "  has_c_code: 0 (0.0%)\n",
            "  has_initial_brace: 0 (0.0%)\n",
            "  has_wrong_bit_width: 0 (0.0%)\n",
            "  has_mixed_comments: 0 (0.0%)\n",
            "  wire_assignment: 11 (28.9%)\n",
            "\n",
            "Clean examples: 27 (71.1%)\n",
            "Problematic examples: 11\n",
            "\n",
            "Clean training data saved to train_clean.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what's actually in the training data responses\n",
        "import json\n",
        "\n",
        "with open('data/processed/train/train.jsonl', 'r') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "# Look at a few response formats\n",
        "for i in range(min(3, len(data))):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Example {i} response format:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    response = data[i]['response']\n",
        "    print(response[:500])\n",
        "\n",
        "    # Check for non-Verilog content\n",
        "    if \"```\" in response:\n",
        "        print(\"⚠️  Contains markdown\")\n",
        "    if \"Change:\" in response or \"###\" in response:\n",
        "        print(\"⚠️  Contains comments/analysis\")\n",
        "    if \"int main\" in response:\n",
        "        print(\"⚠️  Contains C code\")"
      ],
      "metadata": {
        "id": "CMojpglr_SAm",
        "outputId": "479042d3-181d-4ec9-fecd-ea322e47f0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Example 0 response format:\n",
            "============================================================\n",
            "```verilog\n",
            "`timescale 1ns / 1ps\n",
            "\n",
            "module testbench;\n",
            "    reg [7:0] a, b, c, d;\n",
            "    wire [7:0] min;\n",
            "    integer file;\n",
            "\n",
            "    // Instantiate the Device Under Test (DUT)\n",
            "    top_module DUT (\n",
            "        .a(a),\n",
            "        .b(b),\n",
            "        .c(c),\n",
            "        .d(d),\n",
            "        .min(min)\n",
            "    );\n",
            "\n",
            "    initial begin\n",
            "        // Open the file for output\n",
            "        file = $fopen(\"TBout.txt\", \"w\");\n",
            "\n",
            "        // Scenario 1: All inputs are equal\n",
            "        a = 127; b = 127; c = 127; d = 127;\n",
            "        #10; // Wait for the output to stabili\n",
            "⚠️  Contains markdown\n",
            "\n",
            "============================================================\n",
            "Example 1 response format:\n",
            "============================================================\n",
            "```verilog\n",
            "`timescale 1ns / 1ps\n",
            "module testbench;\n",
            "reg  clk;\n",
            "reg  d;\n",
            "reg  ar;\n",
            "wire  q;\n",
            "\n",
            "integer file, scenario;\n",
            "// DUT instantiation\n",
            "top_module DUT (\n",
            "    .clk(clk),\n",
            "    .d(d),\n",
            "    .ar(ar),\n",
            "    .q(q)\n",
            ");\n",
            "// Clock generation\n",
            "initial begin\n",
            "    clk = 0;\n",
            "    forever #5 clk = ~clk;\n",
            "end\n",
            "\n",
            "initial begin\n",
            "    file = $fopen(\"TBout.txt\", \"w\");\n",
            "end\n",
            "// Scenario Based Test\n",
            "initial begin\n",
            "    // Scenario 1\n",
            "    scenario = 1;\n",
            "    ar = 1; d = 0;\n",
            "    $fdisplay(file, \"scenario: %d, clk = %d, d = %d, ar = %d, q = %d\", sc\n",
            "⚠️  Contains markdown\n",
            "\n",
            "============================================================\n",
            "Example 2 response format:\n",
            "============================================================\n",
            "```verilog\n",
            "`timescale 1ns / 1ps\n",
            "\n",
            "module testbench;\n",
            "    reg [7:0] a, b, c, d;\n",
            "    wire [7:0] min;\n",
            "    integer file;\n",
            "\n",
            "    // Instantiate the Device Under Test (DUT)\n",
            "    top_module DUT (\n",
            "        .a(a),\n",
            "        .b(b),\n",
            "        .c(c),\n",
            "        .d(d),\n",
            "        .min(min)\n",
            "    );\n",
            "\n",
            "    initial begin\n",
            "        // Open the file for output\n",
            "        file = $fopen(\"TBout.txt\", \"w\");\n",
            "\n",
            "        // Scenario 1: All inputs are equal\n",
            "        a = 127; b = 127; c = 127; d = 127;\n",
            "        #10; // Wait for the output to stabili\n",
            "⚠️  Contains markdown\n"
          ]
        }
      ]
    }
  ]
}