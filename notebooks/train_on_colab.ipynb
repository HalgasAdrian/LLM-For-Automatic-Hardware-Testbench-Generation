{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HalgasAdrian/LLM-For-Automatic-Hardware-Testbench-Generation/blob/DataPipeLineOptimization/notebooks/train_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# LLM Testbench Generation Training on Colab This notebook trains the model on Google Colab with GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOwjhyHGa2L_",
        "outputId": "fb96c2ea-384c-46c0-c3c9-f1e1830c5f96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Aug 13 01:47:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0             47W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_header"
      },
      "source": [
        "## 3. Upload and Extract Project Files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Upload from local computer\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Select llm_testbench_colab.zip\n",
        "\n",
        "# Extract\n",
        "!unzip -q llm_testbench_colab.zip\n",
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "87JYqbv5bLCm",
        "outputId": "18bc3422-5c01-49b1-e8aa-8983abb04a32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-21dc1d83-7019-4ad2-87e0-bd095c592989\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-21dc1d83-7019-4ad2-87e0-bd095c592989\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving llm_testbench_colab.zip to llm_testbench_colab.zip\n",
            "total 200\n",
            "drwxr-xr-x 1 root root   4096 Aug 13 01:48 .\n",
            "drwxr-xr-x 1 root root   4096 Aug 13 01:41 ..\n",
            "drwxr-xr-x 4 root root   4096 Aug 11 13:35 .config\n",
            "drwxr-xr-x 2 root root   4096 Aug 13 01:48 configs\n",
            "drwxr-xr-x 3 root root   4096 Aug 13 01:48 data\n",
            "-rw-r--r-- 1 root root    177 Aug 10 23:07 .env\n",
            "-rw-r--r-- 1 root root 152773 Aug 13 01:48 llm_testbench_colab.zip\n",
            "drwxr-xr-x 2 root root   4096 Aug 13 01:48 notebooks\n",
            "-rw-r--r-- 1 root root    159 Aug 12 21:47 requirements_colab.txt\n",
            "-rw-r--r-- 1 root root    736 Aug 10 23:33 requirements.txt\n",
            "drwxr-xr-x 1 root root   4096 Aug 11 13:36 sample_data\n",
            "drwxr-xr-x 2 root root   4096 Aug 13 01:48 scripts\n",
            "drwxr-xr-x 3 root root   4096 Aug 13 01:48 utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_header"
      },
      "source": [
        "## 4. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requirements\n",
        "!pip install -q -r requirements_colab.txt\n",
        "\n",
        "# Verify installations\n",
        "!pip list | grep -E \"torch|transformers|peft|bitsandbytes\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsBAILCsbSE0",
        "outputId": "6ba24123-167e-44ab-a265-282648a9d7e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hbitsandbytes                          0.47.0\n",
            "peft                                  0.17.0\n",
            "sentence-transformers                 5.0.0\n",
            "torch                                 2.6.0+cu124\n",
            "torchao                               0.10.0\n",
            "torchaudio                            2.6.0+cu124\n",
            "torchdata                             0.11.0\n",
            "torchsummary                          1.5.1\n",
            "torchtune                             0.6.1\n",
            "torchvision                           0.21.0+cu124\n",
            "transformers                          4.55.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wandb_header"
      },
      "source": [
        "## 5. Configure Weights & Biases (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_wandb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "0f6a1d0b-2af3-4a2c-9b0f-21c7024be89a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhalgas-a\u001b[0m (\u001b[33mhalgas-a-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250813_015038-qzmux3x4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/qzmux3x4' target=\"_blank\">colab-training-run</a></strong> to <a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen' target=\"_blank\">https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/qzmux3x4' target=\"_blank\">https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/qzmux3x4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/qzmux3x4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e83e03b35d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Set up wandb for experiment tracking\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "# Set API key directly\n",
        "os.environ['WANDB_API_KEY'] = 'b29c0d2102aa226ead1fded36b786769f969d4f7'\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"llm-testbench-gen\",\n",
        "    name=\"colab-training-run\",\n",
        "    config={\n",
        "        \"model\": \"TinyLlama-1.1B\",\n",
        "        \"dataset\": \"AutoBench\",\n",
        "        \"epochs\": 30\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_header"
      },
      "source": [
        "## 6. Update Config for Colab GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "update_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a316e6dc-395a-4c72-f470-96b6f0f862fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config fixed and saved to config_colab.yaml\n",
            "eval_steps: 24 (every epoch)\n",
            "save_steps: 48 (every 2 epochs - multiple of eval_steps)\n"
          ]
        }
      ],
      "source": [
        "# Create optimized config for Colab GPU training with 24 clean examples\n",
        "import yaml\n",
        "\n",
        "# Load existing config\n",
        "with open('configs/config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Model settings - optimized for GPU\n",
        "config['model']['quantization']['load_in_4bit'] = True\n",
        "\n",
        "# Training parameters for small dataset (24 examples)\n",
        "config['training']['per_device_train_batch_size'] = 1\n",
        "config['training']['per_device_eval_batch_size'] = 1\n",
        "config['training']['gradient_accumulation_steps'] = 4\n",
        "config['training']['num_train_epochs'] = 30\n",
        "config['training']['learning_rate'] = 5e-5\n",
        "config['training']['warmup_steps'] = 20\n",
        "config['training']['warmup_ratio'] = 0.1\n",
        "config['training']['max_grad_norm'] = 0.5\n",
        "config['training']['fp16'] = True\n",
        "\n",
        "# LoRA configuration\n",
        "config['model']['lora']['r'] = 16\n",
        "config['model']['lora']['lora_alpha'] = 32\n",
        "config['model']['lora']['lora_dropout'] = 0.05\n",
        "config['model']['lora']['target_modules'] = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "\n",
        "# Training schedule - FIXED: save_steps must be multiple of eval_steps\n",
        "config['training']['logging_steps'] = 5\n",
        "config['training']['eval_steps'] = 24        # Evaluate every epoch (24 examples / batch_size 1)\n",
        "config['training']['save_steps'] = 48        # Save every 2 epochs (must be multiple of eval_steps)\n",
        "config['training']['save_total_limit'] = 5\n",
        "config['training']['load_best_model_at_end'] = True\n",
        "config['training']['metric_for_best_model'] = 'eval_loss'\n",
        "config['training']['greater_is_better'] = False\n",
        "\n",
        "# Optimizer and scheduler\n",
        "config['training']['optim'] = 'adamw_torch'\n",
        "config['training']['lr_scheduler_type'] = 'constant_with_warmup'\n",
        "\n",
        "# Generation settings\n",
        "config['generation']['max_new_tokens'] = 4096\n",
        "config['generation']['temperature'] = 0.7\n",
        "config['generation']['top_p'] = 0.95\n",
        "config['generation']['do_sample'] = True\n",
        "\n",
        "# Data settings\n",
        "config['data']['max_testbench_length'] = 4096\n",
        "\n",
        "# Save updated config\n",
        "with open('configs/config_colab.yaml', 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "\n",
        "print(\"Config fixed and saved to config_colab.yaml\")\n",
        "print(f\"eval_steps: {config['training']['eval_steps']} (every epoch)\")\n",
        "print(f\"save_steps: {config['training']['save_steps']} (every 2 epochs - multiple of eval_steps)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_header"
      },
      "source": [
        "## 7. Run Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the train.py file directly in Colab\n",
        "!sed -i 's/evaluation_strategy=/eval_strategy=/g' /content/scripts/train.py\n",
        "\n",
        "# Verify the change was made\n",
        "!grep -n \"eval_strategy\" /content/scripts/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EewVbGdvfBD1",
        "outputId": "aa618d62-1424-4047-a85d-97818c650814"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "159:        eval_strategy=\"steps\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_training",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7695d6-de56-43da-d92f-5aeb2f7cc735"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-13 01:52:53.096066: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-13 01:52:53.113735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755049973.134846    3309 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755049973.141302    3309 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755049973.157900    3309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755049973.157937    3309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755049973.157941    3309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755049973.157944    3309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-13 01:52:53.162688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhalgas-a\u001b[0m (\u001b[33mhalgas-a-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250813_015258-mxoolisi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrain-TinyLlama-1.1B-Chat-v1.0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/halgas-a-northeastern-university/llm-testbench-gen/runs/mxoolisi\u001b[0m\n",
            "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n",
            "/content/scripts/train.py:217: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "  0% 0/180 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "{'loss': 0.63, 'grad_norm': 0.880845308303833, 'learning_rate': 1e-05, 'epoch': 0.83}\n",
            "{'loss': 0.595, 'grad_norm': 0.6697667241096497, 'learning_rate': 2.25e-05, 'epoch': 1.67}\n",
            "{'loss': 0.6938, 'grad_norm': 0.8464495539665222, 'learning_rate': 3.5e-05, 'epoch': 2.5}\n",
            "{'loss': 0.5511, 'grad_norm': 0.6742605566978455, 'learning_rate': 4.75e-05, 'epoch': 3.33}\n",
            " 13% 24/180 [01:00<06:28,  2.49s/it]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00, 11.54it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  7.19it/s]\u001b[A\n",
            " 83% 5/6 [00:00<00:00,  6.67it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8651596903800964, 'eval_runtime': 3.1674, 'eval_samples_per_second': 1.894, 'eval_steps_per_second': 1.894, 'epoch': 4.0}\n",
            " 13% 24/180 [01:03<06:28,  2.49s/it]\n",
            "100% 6/6 [00:02<00:00,  6.38it/s]\u001b[A\n",
            "{'loss': 0.482, 'grad_norm': 0.4048629701137543, 'learning_rate': 5e-05, 'epoch': 4.17}\n",
            "{'loss': 0.5075, 'grad_norm': 0.325234591960907, 'learning_rate': 5e-05, 'epoch': 5.0}\n",
            "{'loss': 0.3984, 'grad_norm': 0.34434953331947327, 'learning_rate': 5e-05, 'epoch': 5.83}\n",
            "{'loss': 0.4177, 'grad_norm': 0.48256897926330566, 'learning_rate': 5e-05, 'epoch': 6.67}\n",
            "{'loss': 0.2906, 'grad_norm': 0.6233924031257629, 'learning_rate': 5e-05, 'epoch': 7.5}\n",
            " 27% 48/180 [02:03<05:29,  2.49s/it]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00, 11.45it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  7.21it/s]\u001b[A\n",
            " 83% 5/6 [00:00<00:00,  6.66it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8015466332435608, 'eval_runtime': 3.1708, 'eval_samples_per_second': 1.892, 'eval_steps_per_second': 1.892, 'epoch': 8.0}\n",
            " 27% 48/180 [02:07<05:29,  2.49s/it]\n",
            "100% 6/6 [00:02<00:00,  6.40it/s]\u001b[A\n",
            "{'loss': 0.296, 'grad_norm': 0.8450599908828735, 'learning_rate': 5e-05, 'epoch': 8.33}\n",
            "{'loss': 0.2183, 'grad_norm': 0.4846433997154236, 'learning_rate': 5e-05, 'epoch': 9.17}\n",
            "{'loss': 0.222, 'grad_norm': 0.8202745318412781, 'learning_rate': 5e-05, 'epoch': 10.0}\n",
            "{'loss': 0.1951, 'grad_norm': 1.0033565759658813, 'learning_rate': 5e-05, 'epoch': 10.83}\n",
            "{'loss': 0.1412, 'grad_norm': 0.5903159976005554, 'learning_rate': 5e-05, 'epoch': 11.67}\n",
            " 40% 72/180 [03:07<04:29,  2.49s/it]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00, 11.50it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  7.22it/s]\u001b[A\n",
            " 83% 5/6 [00:00<00:00,  6.65it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8365240097045898, 'eval_runtime': 3.1703, 'eval_samples_per_second': 1.893, 'eval_steps_per_second': 1.893, 'epoch': 12.0}\n",
            " 40% 72/180 [03:11<04:29,  2.49s/it]\n",
            "100% 6/6 [00:02<00:00,  6.38it/s]\u001b[A\n",
            "{'loss': 0.1614, 'grad_norm': 0.9583685398101807, 'learning_rate': 5e-05, 'epoch': 12.5}\n",
            "{'loss': 0.1375, 'grad_norm': 0.9043564796447754, 'learning_rate': 5e-05, 'epoch': 13.33}\n",
            "{'loss': 0.0937, 'grad_norm': 0.3668292164802551, 'learning_rate': 5e-05, 'epoch': 14.17}\n",
            "{'loss': 0.1176, 'grad_norm': 1.1883056163787842, 'learning_rate': 5e-05, 'epoch': 15.0}\n",
            "{'loss': 0.0789, 'grad_norm': 0.5487140417098999, 'learning_rate': 5e-05, 'epoch': 15.83}\n",
            " 53% 96/180 [04:11<03:29,  2.49s/it]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00, 11.49it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  7.21it/s]\u001b[A\n",
            " 83% 5/6 [00:00<00:00,  6.66it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.9009735584259033, 'eval_runtime': 3.1708, 'eval_samples_per_second': 1.892, 'eval_steps_per_second': 1.892, 'epoch': 16.0}\n",
            " 53% 96/180 [04:14<03:29,  2.49s/it]\n",
            "100% 6/6 [00:02<00:00,  6.40it/s]\u001b[A\n",
            "{'loss': 0.0648, 'grad_norm': 0.3391683101654053, 'learning_rate': 5e-05, 'epoch': 16.67}\n",
            "{'loss': 0.0831, 'grad_norm': 0.8560659289360046, 'learning_rate': 5e-05, 'epoch': 17.5}\n",
            "{'loss': 0.0392, 'grad_norm': 0.2874213457107544, 'learning_rate': 5e-05, 'epoch': 18.33}\n",
            "{'loss': 0.0714, 'grad_norm': 0.38477659225463867, 'learning_rate': 5e-05, 'epoch': 19.17}\n",
            "{'loss': 0.0412, 'grad_norm': 0.4007033109664917, 'learning_rate': 5e-05, 'epoch': 20.0}\n",
            " 67% 120/180 [05:15<02:29,  2.49s/it]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00, 11.44it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 83% 5/6 [00:00<00:00,  6.65it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9694667458534241, 'eval_runtime': 3.1556, 'eval_samples_per_second': 1.901, 'eval_steps_per_second': 1.901, 'epoch': 20.0}\n",
            " 67% 120/180 [05:18<02:29,  2.49s/it]\n",
            "100% 6/6 [00:02<00:00,  6.40it/s]\u001b[A\n",
            "{'loss': 0.0436, 'grad_norm': 0.4305365979671478, 'learning_rate': 5e-05, 'epoch': 20.83}\n",
            "{'loss': 0.045, 'grad_norm': 0.4274352192878723, 'learning_rate': 5e-05, 'epoch': 21.67}\n",
            "{'loss': 0.0338, 'grad_norm': 0.41164112091064453, 'learning_rate': 5e-05, 'epoch': 22.5}\n",
            "{'loss': 0.0309, 'grad_norm': 0.45368722081184387, 'learning_rate': 5e-05, 'epoch': 23.33}\n",
            " 80% 144/180 [06:18<01:29,  2.50s/it]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00, 11.47it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 83% 5/6 [00:00<00:00,  6.65it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0461567640304565, 'eval_runtime': 3.181, 'eval_samples_per_second': 1.886, 'eval_steps_per_second': 1.886, 'epoch': 24.0}\n",
            " 80% 144/180 [06:21<01:29,  2.50s/it]\n",
            "100% 6/6 [00:02<00:00,  6.39it/s]\u001b[A\n",
            "{'loss': 0.0271, 'grad_norm': 0.38145336508750916, 'learning_rate': 5e-05, 'epoch': 24.17}\n",
            "{'loss': 0.0264, 'grad_norm': 0.6506151556968689, 'learning_rate': 5e-05, 'epoch': 25.0}\n",
            "{'loss': 0.0228, 'grad_norm': 0.4350234866142273, 'learning_rate': 5e-05, 'epoch': 25.83}\n",
            "{'loss': 0.0233, 'grad_norm': 0.7391727566719055, 'learning_rate': 5e-05, 'epoch': 26.67}\n",
            "{'loss': 0.029, 'grad_norm': 0.46761518716812134, 'learning_rate': 5e-05, 'epoch': 27.5}\n",
            " 93% 168/180 [07:22<00:29,  2.49s/it]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00, 11.47it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 83% 5/6 [00:00<00:00,  6.65it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0205384492874146, 'eval_runtime': 3.1646, 'eval_samples_per_second': 1.896, 'eval_steps_per_second': 1.896, 'epoch': 28.0}\n",
            " 93% 168/180 [07:25<00:29,  2.49s/it]\n",
            "100% 6/6 [00:02<00:00,  6.39it/s]\u001b[A\n",
            "{'loss': 0.0123, 'grad_norm': 0.6319339871406555, 'learning_rate': 5e-05, 'epoch': 28.33}\n",
            "{'loss': 0.0212, 'grad_norm': 0.9741848111152649, 'learning_rate': 5e-05, 'epoch': 29.17}\n",
            "{'loss': 0.0162, 'grad_norm': 0.4243369698524475, 'learning_rate': 5e-05, 'epoch': 30.0}\n",
            "{'train_runtime': 476.1087, 'train_samples_per_second': 1.512, 'train_steps_per_second': 0.378, 'train_loss': 0.19052609900633494, 'epoch': 30.0}\n",
            "100% 180/180 [07:56<00:00,  2.64s/it]\n",
            "\n",
            "==================================================\n",
            "Training Complete!\n",
            "==================================================\n",
            "Final loss: 0.19052609900633494\n",
            "Total training time: 476.1087 seconds\n",
            "\n",
            "Next step: Run 'python scripts/evaluate.py' to evaluate the model\n"
          ]
        }
      ],
      "source": [
        "# Set config file\n",
        "!cp configs/config_colab.yaml configs/config.yaml\n",
        "\n",
        "# Run training\n",
        "!python scripts/train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_header"
      },
      "source": [
        "## 8. Save Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compress_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adb7cef-344b-4314-8dfa-4ca677cfc6a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: adapter_config.json (deflated 57%)\n",
            "  adding: adapter_model.safetensors (deflated 8%)\n",
            "  adding: chat_template.jinja (deflated 60%)\n",
            "  adding: checkpoint-144/ (stored 0%)\n",
            "  adding: checkpoint-144/tokenizer_config.json (deflated 69%)\n",
            "  adding: checkpoint-144/adapter_model.safetensors (deflated 8%)\n",
            "  adding: checkpoint-144/optimizer.pt (deflated 8%)\n",
            "  adding: checkpoint-144/README.md (deflated 66%)\n",
            "  adding: checkpoint-144/scaler.pt (deflated 60%)\n",
            "  adding: checkpoint-144/trainer_state.json (deflated 79%)\n",
            "  adding: checkpoint-144/tokenizer.model (deflated 55%)\n",
            "  adding: checkpoint-144/adapter_config.json (deflated 57%)\n",
            "  adding: checkpoint-144/rng_state.pth (deflated 25%)\n",
            "  adding: checkpoint-144/special_tokens_map.json (deflated 73%)\n",
            "  adding: checkpoint-144/chat_template.jinja (deflated 60%)\n",
            "  adding: checkpoint-144/training_args.bin (deflated 51%)\n",
            "  adding: checkpoint-144/scheduler.pt (deflated 57%)\n",
            "  adding: checkpoint-144/tokenizer.json (deflated 85%)\n",
            "  adding: checkpoint-180/ (stored 0%)\n",
            "  adding: checkpoint-180/tokenizer_config.json (deflated 69%)\n",
            "  adding: checkpoint-180/adapter_model.safetensors (deflated 8%)\n",
            "  adding: checkpoint-180/optimizer.pt (deflated 8%)\n",
            "  adding: checkpoint-180/README.md (deflated 66%)\n",
            "  adding: checkpoint-180/scaler.pt (deflated 60%)\n",
            "  adding: checkpoint-180/trainer_state.json (deflated 80%)\n",
            "  adding: checkpoint-180/tokenizer.model (deflated 55%)\n",
            "  adding: checkpoint-180/adapter_config.json (deflated 57%)\n",
            "  adding: checkpoint-180/rng_state.pth (deflated 25%)\n",
            "  adding: checkpoint-180/special_tokens_map.json (deflated 73%)\n",
            "  adding: checkpoint-180/chat_template.jinja (deflated 60%)\n",
            "  adding: checkpoint-180/training_args.bin (deflated 51%)\n",
            "  adding: checkpoint-180/scheduler.pt (deflated 57%)\n",
            "  adding: checkpoint-180/tokenizer.json (deflated 85%)\n",
            "  adding: checkpoint-48/ (stored 0%)\n",
            "  adding: checkpoint-48/tokenizer_config.json (deflated 69%)\n",
            "  adding: checkpoint-48/adapter_model.safetensors (deflated 8%)\n",
            "  adding: checkpoint-48/optimizer.pt (deflated 8%)\n",
            "  adding: checkpoint-48/README.md (deflated 66%)\n",
            "  adding: checkpoint-48/scaler.pt (deflated 60%)\n",
            "  adding: checkpoint-48/trainer_state.json (deflated 71%)\n",
            "  adding: checkpoint-48/tokenizer.model (deflated 55%)\n",
            "  adding: checkpoint-48/adapter_config.json (deflated 57%)\n",
            "  adding: checkpoint-48/rng_state.pth (deflated 25%)\n",
            "  adding: checkpoint-48/special_tokens_map.json (deflated 73%)\n",
            "  adding: checkpoint-48/chat_template.jinja (deflated 60%)\n",
            "  adding: checkpoint-48/training_args.bin (deflated 51%)\n",
            "  adding: checkpoint-48/scheduler.pt (deflated 57%)\n",
            "  adding: checkpoint-48/tokenizer.json (deflated 85%)\n",
            "  adding: checkpoint-96/ (stored 0%)\n",
            "  adding: checkpoint-96/tokenizer_config.json (deflated 69%)\n",
            "  adding: checkpoint-96/adapter_model.safetensors (deflated 8%)\n",
            "  adding: checkpoint-96/optimizer.pt (deflated 8%)\n",
            "  adding: checkpoint-96/README.md (deflated 66%)\n",
            "  adding: checkpoint-96/scaler.pt (deflated 60%)\n",
            "  adding: checkpoint-96/trainer_state.json (deflated 76%)\n",
            "  adding: checkpoint-96/tokenizer.model (deflated 55%)\n",
            "  adding: checkpoint-96/adapter_config.json (deflated 57%)\n",
            "  adding: checkpoint-96/rng_state.pth (deflated 25%)\n",
            "  adding: checkpoint-96/special_tokens_map.json (deflated 73%)\n",
            "  adding: checkpoint-96/chat_template.jinja (deflated 60%)\n",
            "  adding: checkpoint-96/training_args.bin (deflated 51%)\n",
            "  adding: checkpoint-96/scheduler.pt (deflated 57%)\n",
            "  adding: checkpoint-96/tokenizer.json (deflated 85%)\n",
            "  adding: README.md (deflated 66%)\n",
            "  adding: special_tokens_map.json (deflated 73%)\n",
            "  adding: tokenizer_config.json (deflated 69%)\n",
            "  adding: tokenizer.json (deflated 85%)\n",
            "  adding: tokenizer.model (deflated 55%)\n",
            "  adding: training_args.bin (deflated 51%)\n",
            "  adding: training_results.json (deflated 34%)\n",
            "-rw-r--r-- 1 root root 210M Aug 13 02:02 trained_model.zip\n"
          ]
        }
      ],
      "source": [
        "# Compress the trained model\n",
        "!cd models/checkpoints && zip -r /content/trained_model.zip * && cd /content\n",
        "!ls -lh trained_model.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "922baa9f-966d-4b8c-8bde-7993cf2e785f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_828ae096-e8ae-4b44-9f2e-e3abda6f1e8e\", \"trained_model.zip\", 220097268)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download to local computer\n",
        "from google.colab import files\n",
        "files.download('trained_model.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Install Icarus Verilog for Testing\n"
      ],
      "metadata": {
        "id": "ZqeA-KLIqClN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Icarus Verilog for compilation tests\n",
        "!apt-get update\n",
        "!apt-get install -y iverilog\n",
        "!iverilog -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_jBGOAFqCCC",
        "outputId": "ef74ccf2-4e1c-4864-b7aa-9564ff7c9418"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,929 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,111 kB]\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,520 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,209 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,575 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,270 kB]\n",
            "Fetched 17.0 MB in 5s (3,393 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  gtkwave\n",
            "The following NEW packages will be installed:\n",
            "  iverilog\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 2,130 kB of archives.\n",
            "After this operation, 6,749 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 iverilog amd64 11.0-1.1 [2,130 kB]\n",
            "Fetched 2,130 kB in 2s (1,313 kB/s)\n",
            "Selecting previously unselected package iverilog.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../iverilog_11.0-1.1_amd64.deb ...\n",
            "Unpacking iverilog (11.0-1.1) ...\n",
            "Setting up iverilog (11.0-1.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "iverilog: invalid option -- 'e'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Run Evaluation"
      ],
      "metadata": {
        "id": "UU_7U4WnqJg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the evaluation script if it doesn't exist\n",
        "!cp /content/scripts/evaluate.py /content/scripts/evaluate_colab.py || echo \"Creating new evaluation script\"\n",
        "\n",
        "# If evaluate.py doesn't exist, download it\n",
        "if not os.path.exists('/content/scripts/evaluate.py'):\n",
        "    print(\"Downloading evaluation script...\")\n",
        "    # You can paste the evaluate.py content here or upload it"
      ],
      "metadata": {
        "id": "RVyPhTRVqKat"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation on the trained model\n",
        "!python scripts/evaluate.py\n",
        "\n",
        "# Check results\n",
        "!ls -la data/test_results/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_K1LfaIrHF-",
        "outputId": "0c7b145a-73a0-4899-eb0a-d1c7918caa35"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-13 02:04:38.992196: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-13 02:04:39.010420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755050679.032033    7133 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755050679.038578    7133 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755050679.055703    7133 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755050679.055737    7133 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755050679.055740    7133 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755050679.055742    7133 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-13 02:04:39.060647: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Evaluating:   0% 0/8 [00:00<?, ?it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
            "Evaluating: 100% 8/8 [15:10<00:00, 113.86s/it]\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "Total examples evaluated: 8\n",
            "\n",
            "Success Rates:\n",
            "  Generation: 100.0%\n",
            "  Compilation: 0.0%\n",
            "  Simulation: 0.0%\n",
            "  Valid Syntax: 0.0%\n",
            "\n",
            "Component Presence:\n",
            "  timescale: 100.0%\n",
            "  module: 100.0%\n",
            "  initial: 100.0%\n",
            "  finish: 50.0%\n",
            "  display: 100.0%\n",
            "\n",
            "Generation Statistics:\n",
            "  Average length: 3184 chars\n",
            "  Min length: 399 chars\n",
            "  Max length: 11115 chars\n",
            "\n",
            "Detailed results saved to: data/test_results\n",
            "\n",
            "Next step: Run 'python scripts/generate.py --dut_file <your_design.v>' to generate testbenches for new designs\n",
            "total 88\n",
            "drwxr-xr-x 2 root root  4096 Aug 13 02:19 .\n",
            "drwxr-xr-x 4 root root  4096 Aug 13 02:19 ..\n",
            "-rw-r--r-- 1 root root 35456 Aug 13 02:19 evaluation_results.json\n",
            "-rw-r--r-- 1 root root   627 Aug 13 02:19 generated_tb_0.v\n",
            "-rw-r--r-- 1 root root   412 Aug 13 02:19 generated_tb_1.v\n",
            "-rw-r--r-- 1 root root   769 Aug 13 02:19 generated_tb_2.v\n",
            "-rw-r--r-- 1 root root 11115 Aug 13 02:19 generated_tb_3.v\n",
            "-rw-r--r-- 1 root root   541 Aug 13 02:19 generated_tb_4.v\n",
            "-rw-r--r-- 1 root root  3978 Aug 13 02:19 generated_tb_5.v\n",
            "-rw-r--r-- 1 root root  7634 Aug 13 02:19 generated_tb_6.v\n",
            "-rw-r--r-- 1 root root   399 Aug 13 02:19 generated_tb_7.v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and display evaluation results\n",
        "import json\n",
        "\n",
        "with open('data/test_results/evaluation_results.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"=\"*50)\n",
        "for metric, value in results['metrics'].items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{metric}: {value:.2%}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xxv7DQOrJsA",
        "outputId": "9b10063c-7784-4564-d3b9-338d0ff61b64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics:\n",
            "==================================================\n",
            "total_examples: 8\n",
            "generation_success_rate: 100.00%\n",
            "compilation_success_rate: 0.00%\n",
            "simulation_success_rate: 0.00%\n",
            "syntax_valid_rate: 0.00%\n",
            "has_timescale_rate: 100.00%\n",
            "has_module_rate: 100.00%\n",
            "has_initial_rate: 100.00%\n",
            "has_finish_rate: 50.00%\n",
            "has_display_rate: 100.00%\n",
            "avg_generation_length: 318437.50%\n",
            "min_generation_length: 399\n",
            "max_generation_length: 11115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat data/test_results/generated_tb_3.v | tail -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI0uwZNaE1BR",
        "outputId": "d0d9a543-a68c-47ca-d03b-83c0f40ba3b2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end\n",
            "enddingpping,\n",
            "endpinde. display display display anddcedcdnden for for forsscndtnd forgeddponeptneckdeneptdnentponcndeponepeptt\n",
            "endnedding.block\n",
            "end\n",
            "endingen\n",
            "endum.displayencingcentialxydottenningdponderpontytestdnreddedponencing:sl\n",
            "ending.validssponnptdpondndnydpondtansspttedcding showdt\n",
            "endd\n",
            "endedptdyebd\n",
            "endpadpoptdeddenponlrtentenponpocpadnmdpolbst\n",
            "endeddpasspaddingdndenfttionpodedding_triggercecsleddeddeddedtedcirstpeptdingdingponponnsldpezzpoftledptfepept_shouldmingtingtingctyeburmingppcmm_valid, date_first_delay_display_pppsspon cptponponppedredentialmul\n",
            "ended underningptential.portalnesp\n",
            "endresetptcmm#logic delay commenting on display displayned show display display showninging (step or under showpping show. delayled.display tra potization (soling_first dis serdss ser c display sync (suppdffe wire block display;\n",
            "flag_dedd_fixed ifd_sttion show and counter display timty display display countere simulation counterring `d `test wire_sim display timing for for display should timing should not must should dut d hit `followe reset d timeuv d_rand d_fixed delay witheuv display stn `clocking top dn using clock simd ddssd show_display comment dot show chron simd d # display det initial sign ent d d show fl final det c det 2 final tim final reverse d_d d #fort delay f d under timuv ver duv #d # dotd d r demo d# d d mill_rs d wait d d d wait\n",
            "endd tim ver dd d t d d d d d display demo pro d d d d d d ddd d tb no d d d tdmu tim dddd di dd no d d dtim test d dd delay\n",
            "endd d d dd\n",
            "enddssdledd d\n",
            "endd d d d d generated no dmu d d generated test d begin\n",
            "dddddd d d d d d must should added `d generated dd dd d d d dot d d d d should must d pro d test test test test test sync ver test test prov d d d test d test tim d top d tim pro cl d d hit top reg d d d d d d d d d d `dd d d d d ddd d begin\n",
            "cl ar d d d d d d tim d d d d cl d d d d d ddd cld d d d d d d dd d_d -> must d ddd d d ar pressed d d ddd_d ar -> d d_ddptddddd d d d:d:d_d d ~d d d d d d d d d d d d d d d d d d_dd cl d d d ard_ard_dddar arposed d_d arranged d d detdrt\n",
            "ended gied entered d arddd_d d d_closed d_ ch cld_dcld_dtdd d d dddd_d d d dtdddd dd d to d to ar to d d_d asd asd d as as asdto d checked d du d~to d under d dut ddd `d ddud ddd d tim dut ddud d underdtduv_tdddmuiled ledledd;\n",
            "d no d u_d d d led `begt d_toto to as as d_d d ddddd_wd d d_d d d d d p d duv_times d dut dut tim delayed waited dut dtd d d dut d dut dutd_drd d instant finished dut;\n",
            "d d dot d cl d d d dot d d d. as as required required ar u u ard circ d d d d d `d for test d fall c_d d wd d no d d d d d_d~d wait d dwd cuv;\n",
            "d;\n",
            "d d d d d d d d fin u;\n",
            "u d d d to to u for for;\n",
            "t as as as as as as asd as as as delay `d `;\n",
            "out out d dout d;\n",
            "d;\n",
            "d d d`dtu_done resolved d_d;\n",
            "dud u;\n",
            "diled ud u d done_timd_valid to;\n",
            "ttdud u as as u asu by asttudt tt d delay;\n",
            "tddd d d dar &u dar aru ard dar du dartdot u tu du tu artd ardt uuu u~u uu u aruuu;\n",
            "u aruled utttdtdt~tledttu artttddt uuuu udtd u ud du art u aruu u uu u d u u u uu;\n",
            "uuu art uu u:d:u d daru,u ard, u du d u;\n",
            "u d du u u aru;\n",
            "du u;\n",
            "u;\n",
            "u uu;\n",
            "u uu d;\n",
            "d;\n",
            "u;\n",
            "u;\n",
            "u;\n",
            "u u;\n",
            "u;\n",
            "d u d;\n",
            "u"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat data/test_results/generated_tb_7.v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrwfr7QPE_YD",
        "outputId": "0d2b8ec2-4c93-4737-d4c2-47b862096951"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`timescale 1ns / 1ps module top_module_tb;\n",
            "reg [7:0] a, b, c, d;\n",
            "wire [7:0] min;\n",
            "top_module uut ( .a(a), .b(b), .c(c), .d(d), .min(min) );\n",
            "initial begin\n",
            "a = 10;\n",
            "b = 5;\n",
            "c = 10;\n",
            "d = 20;\n",
            "$display(\"[TEST] %d %d %d %d %d\", a, b, c, d, min);\n",
            "#10;\n",
            "$finish;\n",
            "endinitial begin\n",
            "#10;\n",
            "endinitial begin\n",
            "a = 11;\n",
            "b = 6;\n",
            "c = 11;\n",
            "d = 21;\n",
            "$display(\"[TEST] %d %d %d %d %d\", a, b, c, d, min);\n",
            "#10;\n",
            "$finish;\n",
            "end\n",
            "endmodule"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View a sample generated testbench\n",
        "import random\n",
        "\n",
        "# List generated files\n",
        "import glob\n",
        "tb_files = glob.glob('data/test_results/generated_tb_*.v')\n",
        "\n",
        "if tb_files:\n",
        "    # Pick a random one to display\n",
        "    sample_file = random.choice(tb_files)\n",
        "    print(f\"Viewing: {sample_file}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    with open(sample_file, 'r') as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(\"No generated testbenches found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2YFgpRArLns",
        "outputId": "9549d91d-742e-4ac9-d0ba-4b5e445c5e1e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Viewing: data/test_results/generated_tb_3.v\n",
            "============================================================\n",
            "`timescale 1ns / 1ps module top_module_tb;\n",
            "reg clk, d, ar;\n",
            "wire q;\n",
            "top_module uut( .clk(clk), .d(d), .ar(ar), .q(q) );\n",
            "initial begin\n",
            "clk = 0;\n",
            "ar = 0;\n",
            "forever #10 clk = ~clk;\n",
            "ar = 1;\n",
            "$display(\"Testing ar %b, q=%d\", ar, q);\n",
            "ar = 0;\n",
            "$display(\"Testing ar %b, q=%d\", ar, q);\n",
            "#100;\n",
            "$finish;\n",
            "end\n",
            "endmodule `display;\n",
            "q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// No change q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 0;\n",
            "// This is the original logic, wrongly indicated as a mutation for demonstration q = 1;\n",
            "// This is the original logic, wrongly indicated as a mutation q = 0;\n",
            "q = 1;\n",
            "q = 0;\n",
            "q = 1;\n",
            "q = 0;\n",
            "q = 1;\n",
            "q = 0;\n",
            "q = 1;\n",
            "q;\n",
            "q = 0;\n",
            "q = 1;\n",
            "q = 0;\n",
            "endurance\n",
            "end\n",
            "end\n",
            "end;\n",
            "q = 0;\n",
            "q\n",
            "endq;\n",
            "end\n",
            "end;\n",
            "end;\n",
            "end;\n",
            "end;\n",
            "end;\n",
            "end;\n",
            "end;\n",
            "end\n",
            "end\n",
            "endforename andgate, anddaugh for\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "endfor\n",
            "end\n",
            "endfor\n",
            "end, which\n",
            "endfor\n",
            "enddution for\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end;\n",
            "end\n",
            "end;\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end;\n",
            "end\n",
            "end\n",
            "end\n",
            "end;\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end;\n",
            "end.\n",
            "end;\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end,\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "end\n",
            "endddingd\n",
            "ending for\n",
            "end\n",
            "end\n",
            "end\n",
            "enddmentned parameter blocking showtest test\n",
            "endhard\n",
            "end\n",
            "enddingpping,\n",
            "endpinde. display display display anddcedcdnden for for forsscndtnd forgeddponeptneckdeneptdnentponcndeponepeptt\n",
            "endnedding.block\n",
            "end\n",
            "endingen\n",
            "endum.displayencingcentialxydottenningdponderpontytestdnreddedponencing:sl\n",
            "ending.validssponnptdpondndnydpondtansspttedcding showdt\n",
            "endd\n",
            "endedptdyebd\n",
            "endpadpoptdeddenponlrtentenponpocpadnmdpolbst\n",
            "endeddpasspaddingdndenfttionpodedding_triggercecsleddeddeddedtedcirstpeptdingdingponponnsldpezzpoftledptfepept_shouldmingtingtingctyeburmingppcmm_valid, date_first_delay_display_pppsspon cptponponppedredentialmul\n",
            "ended underningptential.portalnesp\n",
            "endresetptcmm#logic delay commenting on display displayned show display display showninging (step or under showpping show. delayled.display tra potization (soling_first dis serdss ser c display sync (suppdffe wire block display;\n",
            "flag_dedd_fixed ifd_sttion show and counter display timty display display countere simulation counterring `d `test wire_sim display timing for for display should timing should not must should dut d hit `followe reset d timeuv d_rand d_fixed delay witheuv display stn `clocking top dn using clock simd ddssd show_display comment dot show chron simd d # display det initial sign ent d d show fl final det c det 2 final tim final reverse d_d d #fort delay f d under timuv ver duv #d # dotd d r demo d# d d mill_rs d wait d d d wait\n",
            "endd tim ver dd d t d d d d d display demo pro d d d d d d ddd d tb no d d d tdmu tim dddd di dd no d d dtim test d dd delay\n",
            "endd d d dd\n",
            "enddssdledd d\n",
            "endd d d d d generated no dmu d d generated test d begin\n",
            "dddddd d d d d d must should added `d generated dd dd d d d dot d d d d should must d pro d test test test test test sync ver test test prov d d d test d test tim d top d tim pro cl d d hit top reg d d d d d d d d d d `dd d d d d ddd d begin\n",
            "cl ar d d d d d d tim d d d d cl d d d d d ddd cld d d d d d d dd d_d -> must d ddd d d ar pressed d d ddd_d ar -> d d_ddptddddd d d d:d:d_d d ~d d d d d d d d d d d d d d d d d d_dd cl d d d ard_ard_dddar arposed d_d arranged d d detdrt\n",
            "ended gied entered d arddd_d d d_closed d_ ch cld_dcld_dtdd d d dddd_d d d dtdddd dd d to d to ar to d d_d asd asd d as as asdto d checked d du d~to d under d dut ddd `d ddud ddd d tim dut ddud d underdtduv_tdddmuiled ledledd;\n",
            "d no d u_d d d led `begt d_toto to as as d_d d ddddd_wd d d_d d d d d p d duv_times d dut dut tim delayed waited dut dtd d d dut d dut dutd_drd d instant finished dut;\n",
            "d d dot d cl d d d dot d d d. as as required required ar u u ard circ d d d d d `d for test d fall c_d d wd d no d d d d d_d~d wait d dwd cuv;\n",
            "d;\n",
            "d d d d d d d d fin u;\n",
            "u d d d to to u for for;\n",
            "t as as as as as as asd as as as delay `d `;\n",
            "out out d dout d;\n",
            "d;\n",
            "d d d`dtu_done resolved d_d;\n",
            "dud u;\n",
            "diled ud u d done_timd_valid to;\n",
            "ttdud u as as u asu by asttudt tt d delay;\n",
            "tddd d d dar &u dar aru ard dar du dartdot u tu du tu artd ardt uuu u~u uu u aruuu;\n",
            "u aruled utttdtdt~tledttu artttddt uuuu udtd u ud du art u aruu u uu u d u u u uu;\n",
            "uuu art uu u:d:u d daru,u ard, u du d u;\n",
            "u d du u u aru;\n",
            "du u;\n",
            "u;\n",
            "u uu;\n",
            "u uu d;\n",
            "d;\n",
            "u;\n",
            "u;\n",
            "u;\n",
            "u u;\n",
            "u;\n",
            "d u d;\n",
            "u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress evaluation results\n",
        "!cd data/test_results && zip -r /content/evaluation_results.zip * && cd /content\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download('evaluation_results.zip')\n",
        "print(\"Evaluation results downloaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "SblGOIcdtw6w",
        "outputId": "9799ae26-8da8-4fb4-ff8e-68e02ecdd4e2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: evaluation_results.json (deflated 81%)\n",
            "  adding: generated_tb_0.v (deflated 50%)\n",
            "  adding: generated_tb_1.v (deflated 43%)\n",
            "  adding: generated_tb_2.v (deflated 72%)\n",
            "  adding: generated_tb_3.v (deflated 49%)\n",
            "  adding: generated_tb_4.v (deflated 72%)\n",
            "  adding: generated_tb_5.v (deflated 44%)\n",
            "  adding: generated_tb_6.v (deflated 47%)\n",
            "  adding: generated_tb_7.v (deflated 67%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8709cdc4-88a3-42fd-82c5-33397d00fb8f\", \"evaluation_results.zip\", 13103)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze training data quality\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Load training data\n",
        "with open('data/processed/train/train.jsonl', 'r') as f:\n",
        "    train_data = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"Total training examples: {len(train_data)}\")\n",
        "\n",
        "# Check for problematic patterns\n",
        "issues = {\n",
        "    'has_c_code': 0,\n",
        "    'has_initial_brace': 0,\n",
        "    'has_wrong_bit_width': 0,\n",
        "    'has_mixed_comments': 0,\n",
        "    'wire_assignment': 0\n",
        "}\n",
        "\n",
        "clean_examples = []\n",
        "problematic_indices = []\n",
        "\n",
        "for i, example in enumerate(train_data):\n",
        "    tb = example['testbench_code']\n",
        "    has_issue = False\n",
        "\n",
        "    # Check for C code\n",
        "    if 'int main' in tb or '#include' in tb:\n",
        "        issues['has_c_code'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    # Check for wrong initial syntax\n",
        "    if 'initial {' in tb:\n",
        "        issues['has_initial_brace'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    # Check for wrong bit widths\n",
        "    if re.search(r'\\d{2,}\\'b', tb):  # 10'b, 12'b, etc.\n",
        "        issues['has_wrong_bit_width'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    # Check for mixed comments/documentation\n",
        "    if '// Change:' in tb or '### Correctness:' in tb:\n",
        "        issues['has_mixed_comments'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    # Check for wire assignments in initial blocks\n",
        "    if re.search(r'initial.*?wire.*?=', tb, re.DOTALL):\n",
        "        issues['wire_assignment'] += 1\n",
        "        has_issue = True\n",
        "\n",
        "    if has_issue:\n",
        "        problematic_indices.append(i)\n",
        "    else:\n",
        "        clean_examples.append(example)\n",
        "\n",
        "print(\"\\nIssues found in training data:\")\n",
        "for issue, count in issues.items():\n",
        "    print(f\"  {issue}: {count} ({count/len(train_data)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClean examples: {len(clean_examples)} ({len(clean_examples)/len(train_data)*100:.1f}%)\")\n",
        "print(f\"Problematic examples: {len(problematic_indices)}\")\n",
        "\n",
        "# Save clean training data\n",
        "if clean_examples:\n",
        "    with open('data/processed/train/train_clean.jsonl', 'w') as f:\n",
        "        for example in clean_examples:\n",
        "            f.write(json.dumps(example) + '\\n')\n",
        "    print(f\"\\nClean training data saved to train_clean.jsonl\")"
      ],
      "metadata": {
        "id": "K0S_Ht7X-tPC",
        "outputId": "f80fe9de-3897-4dd0-9caf-5d4ce8150097",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training examples: 24\n",
            "\n",
            "Issues found in training data:\n",
            "  has_c_code: 0 (0.0%)\n",
            "  has_initial_brace: 0 (0.0%)\n",
            "  has_wrong_bit_width: 0 (0.0%)\n",
            "  has_mixed_comments: 0 (0.0%)\n",
            "  wire_assignment: 0 (0.0%)\n",
            "\n",
            "Clean examples: 24 (100.0%)\n",
            "Problematic examples: 0\n",
            "\n",
            "Clean training data saved to train_clean.jsonl\n"
          ]
        }
      ]
    }
  ]
}